{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39989d65-c006-44d4-a98c-38dab99c1a4c",
   "metadata": {},
   "source": [
    "## CPSC 474 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b13768-9df9-4167-bf3e-0649beabf996",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ae1c10-b768-4aed-bc0c-08c8f7e89ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE 0.13638115651216065\n",
      "Linear Regression R^2 0.5845057380278651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.30\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m poly_func \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree \u001b[38;5;241m=\u001b[39m n)\n\u001b[0;32m---> 32\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpoly_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m X_test \u001b[38;5;241m=\u001b[39m poly_func\u001b[38;5;241m.\u001b[39mfit_transform(X_test)\n\u001b[1;32m     35\u001b[0m nlr_model \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/preprocessing/_polynomial.py:479\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# XP[:, start:end] are terms of degree d - 1\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# that exclude feature #feature_idx.\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     current_col \u001b[38;5;241m=\u001b[39m next_col\n\u001b[1;32m    487\u001b[0m new_index\u001b[38;5;241m.\u001b[39mappend(current_col)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "data = pd.read_csv(\"Data1.csv\")\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# LinearRegression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "y_preds = lr_model.predict(X_test)\n",
    "\n",
    "print(\"Linear Regression RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "print(\"Linear Regression R^2\", r2_score(y_preds, y_test))\n",
    "\n",
    "\n",
    "# NonLinearRegression Model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "for n in range(17, 25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "    \n",
    "    poly_func = PolynomialFeatures(degree = n)\n",
    "    \n",
    "    X_train = poly_func.fit_transform(X_train)\n",
    "    X_test = poly_func.fit_transform(X_test)\n",
    "    \n",
    "    nlr_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_preds = nlr_model.predict(X_test)\n",
    "\n",
    "    print(f\"Nonlinear Regression w/ Polynomial {n} RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "    print(f\"Nonlinear Regression w/ Polynomial {n} R^2\", r2_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2e799-7c7a-4426-b181-f07ebd866797",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bde5d-1397-4b2c-b9f8-9b3181b00340",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.20\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# LinearRegression Model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m cost, w \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mones((X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)), X_test), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_test, w\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, theta, alpha, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, w\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;66;03m# do prediction\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     cost[i] \u001b[38;5;241m=\u001b[39m cal_cost(w\u001b[38;5;241m.\u001b[39mT, X, y) \u001b[38;5;66;03m# calculate cost\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     w[i, :] \u001b[38;5;241m=\u001b[39m \u001b[43mtheta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cost, w\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "\n",
    "data = pd.read_csv(\"Data1.csv\")\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "def cal_cost(theta, X, y):\n",
    "    m = len(y)\n",
    "\n",
    "    predictions = X.dot(theta)\n",
    "\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, epoch):\n",
    "    # alpha = 0.01\n",
    "    # epoch = 100\n",
    "    X = (X - X.mean()) / X.std() # scale data\n",
    "    \n",
    "    m = X.shape[0] # size of features\n",
    "    X = np.concatenate((np.ones((m, 1)), X), axis=1) # add columns of 1 before X\n",
    "    cost = np.zeros(epoch) # initialize cost\n",
    "    n = X.shape[1]\n",
    "    w = np.zeros(n) # initialize weight\n",
    "\n",
    "    # m = len(y)\n",
    "    # cost = np.zeros(epoch)\n",
    "    # w = np.zeros(epoch, 2)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        yhat = np.dot(X, w.T) # do prediction\n",
    "        cost[i] = cal_cost(w.T, X, y) # calculate cost\n",
    "        w[i, :] = theta.T\n",
    "    \n",
    "    return cost, w\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# LinearRegression Model\n",
    "cost, w = gradient_descent(X_train, y_train, 1e-7, 50000)\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "y_preds = np.dot(X_test, w.T)\n",
    "\n",
    "print(cost[-1], w)\n",
    "print(\"Linear Regression RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "print(\"Linear Regression R^2\", r2_score(y_preds, y_test))\n",
    "\n",
    "# NonLinearRegression Model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "for n in range(2, 12):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "    \n",
    "    poly_func = PolynomialFeatures(degree = n)\n",
    "    \n",
    "    X_train = poly_func.fit_transform(X_train)\n",
    "    X_test = poly_func.fit_transform(X_test)\n",
    "    \n",
    "    cost, w = gradient_descent(X_train, y_train, 0.01, 5000)\n",
    "    X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "    y_preds = np.dot(X_test, w.T)\n",
    "    # print(cost[-1], w)\n",
    "    print(f\"Nonlinear Regression w/ Polynomial {n} RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "    print(f\"Nonlinear Regression w/ Polynomial {n} R^2\", r2_score(y_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8707da-1328-4e11-8094-f62c14d4dfc9",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db89bb0-13dc-4827-b285-80587e2b65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "def linear_least_square_approach(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "    lr_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_preds = lr_model.predict(X_test)\n",
    "\n",
    "    print(\"Linear Regression RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "    print(\"Linear Regression R^2\", r2_score(y_preds, y_test))\n",
    "\n",
    "\n",
    "# NonLinearRegression Model\n",
    "def non_linear_least_square_approach(X_orig, y_orig):\n",
    "\n",
    "    for n in range(2, 10):\n",
    "        X = np.asarray(X_orig)\n",
    "        y = np.asarray(y_orig)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "        poly_func = PolynomialFeatures(degree=n)\n",
    "\n",
    "        X_train = poly_func.fit_transform(X_train)\n",
    "        X_test = poly_func.fit_transform(X_test)\n",
    "\n",
    "        nlr_model = LinearRegression().fit(X_train, y_train)\n",
    "        y_preds = nlr_model.predict(X_test)\n",
    "\n",
    "        print(f\"Nonlinear Regression w/ Polynomial {n} RMSE\", np.sqrt(mse(y_preds, y_test)))\n",
    "        print(f\"Nonlinear Regression w/ Polynomial {n} R^2\", r2_score(y_preds, y_test))\n",
    "\n",
    "\n",
    "# Normalization (minMax Scaler) scaling from 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(\"Data1.csv\")\n",
    "\n",
    "# independent values\n",
    "X = data.iloc[:, 0:-1]\n",
    "# dependent values\n",
    "y = data.iloc[:, -1]\n",
    "print(data)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=['T', 'P', 'TC', 'SV', 'Idx'])\n",
    "print(scaled_data)\n",
    "linear_least_square_approach(X, y)\n",
    "non_linear_least_square_approach(X, y)\n",
    "\n",
    "print(\"\\nUsing Scaled Data\")\n",
    "scaled_x = scaled_data.iloc[:, 0:-1]\n",
    "scaled_y = scaled_data.iloc[:, -1]\n",
    "linear_least_square_approach(scaled_x, scaled_y)\n",
    "non_linear_least_square_approach(scaled_x, scaled_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604d089-bfad-4087-9c64-48b2c76b83b9",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480fd8c-41ed-4a1a-ab66-468b3583fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "\n",
    "data = pd.read_csv(\"Data1.csv\")\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = 0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"score for testing lasso: \",lasso.score(X_test, y_test))\n",
    "print(\"score for training lasso: \",lasso.score(X_train, y_train))\n",
    "\n",
    "y_preds_L = lasso.predict(X_test)\n",
    "\n",
    "print(\"Lasso Regression RMSE\", np.sqrt(mse(y_preds_L, y_test)))\n",
    "print(\"Lasso Regression R^2\", r2_score(y_preds_L, y_test))\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"score for testing ridge: \",ridge.score(X_test, y_test))\n",
    "print(\"score for training ridge: \",ridge.score(X_train, y_train))\n",
    "\n",
    "y_preds_R = ridge.predict(X_test)\n",
    "\n",
    "print(\"Ridge Regression RMSE\", np.sqrt(mse(y_preds_R, y_test)))\n",
    "print(\"Ridge Regression R^2\", r2_score(y_preds_R, y_test))\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "EN = ElasticNet(alpha=0.01)\n",
    "EN.fit(X_train, y_train)\n",
    "\n",
    "print(\"score for testing EN: \",EN.score(X_test, y_test))\n",
    "print(\"score for training EN: \",EN.score(X_train, y_train))\n",
    "\n",
    "y_preds_E = EN.predict(X_test)\n",
    "\n",
    "print(\"EN Regression RMSE\", np.sqrt(mse(y_preds_E, y_test)))\n",
    "print(\"EN Regression R^2\", r2_score(y_preds_E, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e710dc0-d2e2-4bec-83ef-4ba4d34cdc55",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eb06e-2f99-4c47-ac16-385af98e4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Value: 0.116 (0.000)\n",
      "Ridge Value: 0.109 (0.000)\n",
      "EN Value: 0.111 (0.000)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "#data input\n",
    "data = pd.read_csv(\"Data1.csv\")\n",
    "\n",
    "X = data.iloc[:,0:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)\n",
    "\n",
    "#models\n",
    "lasso = Lasso(alpha = 0.1)\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "EN = ElasticNet(alpha = 0.1)\n",
    "\n",
    "#fit\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "EN.fit(X_train, y_train)\n",
    "\n",
    "#evaluation methods\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#cv Test calculation\n",
    "score_Lasso_Test = cross_val_score(lasso, X_test, y_test, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "score_Ridge_Test = cross_val_score(ridge, X_test, y_test, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "score_EN_Test = cross_val_score(EN, X_test, y_test, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "score_Lasso_Test = absolute(score_Lasso_Test)\n",
    "score_Ridge_Test = absolute(score_Ridge_Test)\n",
    "score_EN_Test = absolute(score_EN_Test)\n",
    "\n",
    "#cv Training\n",
    "score_Lasso_Train = cross_val_score(lasso, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "score_Ridge_Train = cross_val_score(ridge, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "score_EN_Train = cross_val_score(EN, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "score_Lasso_Train = absolute(score_Lasso_Train)\n",
    "score_Ridge_Train = absolute(score_Ridge_Train)\n",
    "score_EN_Train = absolute(score_EN_Train)\n",
    "\n",
    "#print test\n",
    "print(\"-----------Test Values-----------\")\n",
    "print(\"Lasso Test Value: %.3f (%.3f)\" % (mean(score_Lasso_Test), std(score_Lasso_Test)))\n",
    "print(\"Ridge Test Value: %.3f (%.3f)\" % (mean(score_Ridge_Test), std(score_Ridge_Test)))\n",
    "print(\"EN Test Value: %.3f (%.3f)\" % (mean(score_EN_Test), std(score_EN_Test)))\n",
    "\n",
    "print()\n",
    "print(\"-----------Train Values-----------\")\n",
    "\n",
    "#print train\n",
    "print(\"Lasso Train Value: %.3f (%.3f)\" % (mean(score_Lasso_Train), std(score_Lasso_Train)))\n",
    "print(\"Ridge Train Value: %.3f (%.3f)\" % (mean(score_Ridge_Train), std(score_Ridge_Train)))\n",
    "print(\"EN Train Value: %.3f (%.3f)\" % (mean(score_EN_Train), std(score_EN_Train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c509bc5-3ca3-4904-8d66-40e7315cea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
